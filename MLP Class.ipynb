{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing an MLP Class from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import metrics\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Percetron Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP():  \n",
    "    \"\"\"\n",
    "    MLP\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, NI = 2, NH = 2, NO = 1):\n",
    "        # defined at start\n",
    "        self.NI = NI\n",
    "        self.NH = NH\n",
    "        self.NO = NO\n",
    "\n",
    "        # no definition required\n",
    "        self.W1 = self.dW1 = np.zeros((self.NH,self.NI)) # extra row wont be used just set to 1\n",
    "        self.W2 = self.dW2 = np.zeros((self.NO,self.NH))\n",
    "        self.Z1 = self.H = np.zeros((self.NH,1))\n",
    "        self.Z2 = np.zeros((self.NO,1))\n",
    "        self.O = np.zeros((self.NO,1))\n",
    "        \n",
    "        # definition provided later\n",
    "        self.I = np.zeros((self.NI,1)) \n",
    "        self.delta1 = np.zeros((self.NH,1))\n",
    "        self.delta2 = np.zeros((self.NO,1))\n",
    "        self.learning_rate = 0.1\n",
    "        self.bias1 = np.zeros((self.NH,1))\n",
    "        self.bias2 = np.zeros((self.NO,1))\n",
    "    \n",
    "    def randomise(self,lower_bound = 0.001, upper_bound=0.4):\n",
    "        for row in range(self.dW1.shape[0]):\n",
    "            for col in range(self.dW1.shape[1]):\n",
    "                self.dW1[row][col] = random.uniform(lower_bound, upper_bound)\n",
    "        \n",
    "        for row in range(self.dW2.shape[0]):\n",
    "            for col in range(self.dW2.shape[1]):\n",
    "                self.dW2[row][col] = random.uniform(lower_bound, upper_bound)\n",
    "                \n",
    "        for row in range(self.bias1.shape[0]):\n",
    "            self.bias1[row] = random.uniform(lower_bound, upper_bound)\n",
    "                \n",
    "        for row in range(self.bias2.shape[0]):\n",
    "            self.bias2[row] = random.uniform(lower_bound, upper_bound)\n",
    "        \n",
    "    def forward(self,example_input):\n",
    "        self.I = np.asarray(example_input) # hopefully NI x 1\n",
    "        self.I.shape = (self.NI,1)\n",
    "        self.Z1 = np.dot(self.W1,self.I) + self.bias1\n",
    "        for row in range(self.Z1.shape[0]):\n",
    "            self.H[row] = 1/(1+math.exp(-self.Z1[row]))\n",
    "        self.H.shape = (self.NH,1)\n",
    "        self.Z2 = np.dot(self.W2,self.H) + self.bias2\n",
    "        for row2 in range(self.Z2.shape[0]):\n",
    "            self.O[row2] = 1/(1+math.exp(-self.Z2[row2]))\n",
    "    \n",
    "    def double_backwards(self,example_output,learning_rate = 0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        y = np.asarray(example_output)\n",
    "        y.shape = (self.NO,1)\n",
    "        self.delta2 =  y - self.O\n",
    "        nearly_delta1 = np.dot(self.W2.transpose(),self.delta2)\n",
    "        for row in range(nearly_delta1[0].shape[0]):\n",
    "            self.delta1[row] = nearly_delta1[row]*self.H[row]*(1-self.H[row])\n",
    "        self.dW2 = self.learning_rate*np.dot(self.delta2,self.H.transpose())\n",
    "        self.dW1 = self.learning_rate*np.dot(self.delta1,self.I.transpose())\n",
    "        error = 0.5*sum(np.power(self.delta2,2))\n",
    "        return error\n",
    "\n",
    "    def update_weights(self):\n",
    "        self.bias1 += self.learning_rate*self.delta1*0.5\n",
    "        self.bias2 += self.learning_rate*self.delta2*0.5\n",
    "        self.W1 += self.dW1\n",
    "        self.W2 += self.dW2\n",
    "        self.dW1 = np.zeros((self.NH,self.NI))\n",
    "        self.dW2 = np.zeros((self.NO,self.NH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing it on XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at epoch 0 is 0.5864738976000785\n",
      "Error at epoch 50000 is 0.2476224121229274\n",
      "Error at epoch 100000 is 0.23609016766041863\n",
      "Error at epoch 150000 is 0.22558145312231193\n",
      "Error at epoch 200000 is 0.2156153921576765\n",
      "Error at epoch 250000 is 0.20612597385813353\n",
      "Error at epoch 300000 is 0.19709069782973102\n",
      "Error at epoch 350000 is 0.1884918850498542\n",
      "Error at epoch 400000 is 0.18031219568918727\n",
      "Error at epoch 450000 is 0.17253420855258195\n",
      "[7.402023987751011e-05, 0.999999999929517, 0.5931050003205008, 0.3638192102836604]\n"
     ]
    }
   ],
   "source": [
    "# XOR\n",
    "examples = [[[0,0],0],[[0,1],1],[[1,0],1],[[1,1],0]]\n",
    "NN = MyMLP(NI=2,NH=2,NO=1)\n",
    "NN.randomise(lower_bound = -1/math.sqrt(2+2), upper_bound = 1/math.sqrt(2+2)) # lb = 0.001, ub = 0.4, lr = 1\n",
    "max_epochs = 500000\n",
    "for e in range(max_epochs):\n",
    "    error = 0\n",
    "    for ex in range(len(examples)):\n",
    "        NN.forward(examples[ex][0])\n",
    "        out = np.asarray(examples[ex][1])\n",
    "        error += NN.double_backwards(out,learning_rate = 0.5)[0]\n",
    "        NN.update_weights()\n",
    "    if e%50000==0: print(\"Error at epoch\",e,\"is\",error)\n",
    "\n",
    "predictions = []\n",
    "for ex in range(len(examples)):\n",
    "    NN.forward(examples[ex][0])\n",
    "    predictions += [np.ndarray.tolist(NN.O)[0][0]]\n",
    "    # should be [0,1,1,0]\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it didn't succeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing it on Letter Recognition Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Letter Recognition Data\n",
    "with open('letter-recognition.data') as myfile:\n",
    "    dataset = myfile.read()\n",
    "\n",
    "dataset = dataset.split(\"\\n\")\n",
    "dataset = dataset[:20000]\n",
    "y_letter = []\n",
    "x = []\n",
    "for i in range(len(dataset)):\n",
    "    row = dataset[i].split(\",\")\n",
    "    y_letter += row[0]\n",
    "    x.append(row[1:])\n",
    "    for entry in enumerate(x[i]):\n",
    "        x[i][entry[0]] = float(int(entry[1]))\n",
    "        \n",
    "x = np.asarray(x)\n",
    "for col in range(x.shape[1]):\n",
    "    maxx = max(x[:,col])\n",
    "    minn = min(x[:,col])\n",
    "    for row in range(x.shape[0]):\n",
    "        x[row,col] = (x[row,col]-minn)/(maxx-minn)\n",
    "y_int = np.zeros((len(y_letter),1),dtype=np.int)\n",
    "y = np.zeros((len(y_letter),26),dtype=np.int)\n",
    "for k in range(len(y_letter)):\n",
    "    y_int[k] = ord(y_letter[k])-65\n",
    "    y[k,y_int[k][0]] = 1\n",
    "\n",
    "randomlist = random.sample(range(20000),20000)\n",
    "\n",
    "NN3 = MyMLP(NI=16,NH=10,NO=26)\n",
    "NN3.randomise(lower_bound = -0.15, upper_bound=0.15)\n",
    "max_epochs = 500\n",
    "for e in range(max_epochs):\n",
    "    error = 0\n",
    "    for ex in range(16000):\n",
    "        NN3.forward(x[randomlist[ex]])\n",
    "        error += NN3.double_backwards(y[randomlist[ex]],learning_rate = 0.4)[0]\n",
    "        NN3.update_weights()\n",
    "    if e%10 == 0: print(\"Error at epoch\",e,\"is\",error)\n",
    "\n",
    "predictions = np.zeros((20000,26))\n",
    "letter_predictions = []\n",
    "prob_predictions = []\n",
    "for ex in range(0,20000):\n",
    "    NN3.forward(x[randomlist[ex]])\n",
    "    col_of_max = np.argmax(np.max(NN3.O, axis=1))\n",
    "    letter_predictions += [chr(col_of_max+65)]\n",
    "    prob_predictions += [NN3.O[col_of_max][0]/sum(NN3.O)[0]]\n",
    "    for col in range(0,26):\n",
    "        predictions[ex][col] = NN3.O[col][0]\n",
    "error = 0.5*sum(sum(np.power(predictions[16000:] - y[randomlist[16000:]],2)))\n",
    "print(error/4000)\n",
    "\n",
    "sorted_letter_predictions = [x for _,x in sorted(zip(randomlist,letter_predictions),\\\n",
    "                                       key=lambda pair: pair[0])]\n",
    "print(\"Classification Acc on Whole Dataset: \",\\\n",
    "      metrics.accuracy_score(y_letter, sorted_letter_predictions)*100, \"%\")\n",
    "\n",
    "num_train_correct = 0\n",
    "num_test_correct = 0\n",
    "for obs in range(16000):\n",
    "    if letter_predictions[obs] == y_letter[randomlist[obs]]:\n",
    "        num_train_correct+=1\n",
    "for obs in range(16000,20000):\n",
    "    if letter_predictions[obs] == y_letter[randomlist[obs]]:\n",
    "        num_test_correct+=1\n",
    "print(\"Classification Acc on Train Set:\",num_train_correct*100/16000,\"%\")\n",
    "print(\"Classification Acc on Test Set:\",num_test_correct*100/4000,\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
