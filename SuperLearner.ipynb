{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a SuperLearner Class and Fitting it on MNIST Fashion Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from random import randrange\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn import neighbors\n",
    "from sklearn import neural_network\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import chain, combinations\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from IPython.display import display, HTML, Image\n",
    "import numpy.random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function that reads in data:\n",
    "Converts csv to dataframe and samples from according to sample rate.\n",
    "Also normalises the X values by dividing by max pixel value (255) - note 0 corresponds to pure white and 255 to pure black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_In_File(filename, sample_rate):\n",
    "    \"\"\"\n",
    "    Reads in file by path and samples from it\n",
    "    Divides all X values (after col 1) by 255 to normalise\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(filename)\n",
    "    dataset = pd.DataFrame(data=dataset)\n",
    "    dataset = dataset.sample(frac=sample_rate)\n",
    "    dataset[dataset.columns[1:]] = dataset[dataset.columns[1:]] /255\n",
    "    return(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function that takes data and no. of folds and produces dict with those folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Divide_Into_Folds(data,num_folds,random_seed=5):\n",
    "    \"\"\"\n",
    "    Takes data and number of folds and returns dict with each entry a fold \n",
    "    \"\"\"\n",
    "    for n in range(1,num_folds+1):\n",
    "        locals()['fold'+str(n)] = []\n",
    "    library_of_folds = dict()\n",
    "    obs_each = data.shape[0]//num_folds # at least how many obs each fold\n",
    "    extra = data.shape[0]%num_folds # spread these extra ones across folds\n",
    "    random.seed(random_seed)\n",
    "    randomlist = random.sample(range(len(data)),len(data))\n",
    "    counter = 0\n",
    "    for n in range(1,extra+1):\n",
    "        locals()['fold'+str(n)] += [data.iloc[row] for row in randomlist[counter:counter+obs_each+1]]\n",
    "        counter = counter+obs_each+1\n",
    "    for n in range(extra+1,num_folds+1):\n",
    "        locals()['fold'+str(n)] += [data.iloc[row] for row in randomlist[counter:counter+obs_each]]\n",
    "        counter = counter+obs_each\n",
    "    for n in range(1,num_folds+1):\n",
    "        locals()['fold'+str(n)] = pd.DataFrame(data = locals()['fold'+str(n)])\n",
    "        library_of_folds[n] = locals()['fold'+str(n)]\n",
    "    return(library_of_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions that returns a model based on some text entry (used for brevity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BaseModelName(name,criterion=\"entropy\",min_samples_split = 200, \\\n",
    "                  min_samples_leaf = 50, max_depth=12, n_neighbors=20):\n",
    "    \"\"\"\n",
    "    Returns model function for string entry specifying model\n",
    "    Can also specify hyperparameters too\n",
    "    \"\"\"\n",
    "    if name == 'dt':\n",
    "        return(tree.DecisionTreeClassifier(criterion=criterion, \\\n",
    "                                           min_samples_split = min_samples_split, \\\n",
    "                                           min_samples_leaf = min_samples_leaf, \\\n",
    "                                           max_depth=max_depth))\n",
    "    if name == 'lr':\n",
    "        return(linear_model.LogisticRegression(multi_class='ovr', C=1, solver='liblinear', max_iter=1000))\n",
    "    elif name == 'knn':\n",
    "        return(neighbors.KNeighborsClassifier(n_neighbors=n_neighbors))\n",
    "    elif name == 'mlp':\n",
    "        return(neural_network.MLPClassifier(hidden_layer_sizes=(300, 100)))\n",
    "    elif name == 'svm':\n",
    "        return(svm.LinearSVC())\n",
    "    elif name == 'nb':\n",
    "        return(naive_bayes.GaussianNB())\n",
    "    else:\n",
    "        return(\"Error: Input Not A Correct Model Name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & Partition Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I read in a different dataset than the normal MNIST train dataset. I read in the first 7500 rows of that dataset, simply because I would get a Memory Error if I attempted to read in the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_data = Read_In_File('fashion-mnist_train-cut.csv', 1)\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "num_classes = 10\n",
    "classes = {0: \"T-shirt/top\", 1:\"Trouser\", 2: \"Pullover\", 3:\"Dress\", 4:\"Coat\", 5:\"Sandal\", 6:\"Shirt\", 7:\"Sneaker\", 8:\"Bag\", 9:\"Ankle boot\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, expectedly the distribution of labels is not exactly distributed equally among the dataset so I ran through each row and deleted it if the count of the label of that row exceeded the minimum label count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for row in range(len(all_data)):\n",
    "    counter = Counter(all_data[\"label\"])\n",
    "    label = all_data[\"label\"][row]\n",
    "    if counter[label] > min(counter.values()):\n",
    "        all_data = all_data.drop(row)\n",
    "        row -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then split the resulting dataset into X and Y datasets, and divided them further into training and test sets, both stratified by the label count in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luke\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X = all_data[all_data.columns[1:]]\n",
    "Y = all_data[all_data.columns[0]]\n",
    "labels = list(Counter(all_data[\"label\"]).keys())\n",
    "X_train, X_test, Y_train, Y_test \\\n",
    "= train_test_split(X, Y, random_state=89, train_size = 0.8, stratify = Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Function that returns a bootstrapped sample. So sample with replacement, and size of sample determined by inputted fraction of size of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Bootstrap(data, size_as_fraction):\n",
    "    \"\"\"\n",
    "    Takes in data (assumes data frame) and returns sample of data \n",
    "    (with size of sample specified by size_as_fraction and sample with replacement)\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    index = numpy.random.randint(0, n, n)\n",
    "    sample = []\n",
    "    sample += [data.iloc[row] for row in index]\n",
    "    sample = pd.DataFrame(sample)\n",
    "    sample = sample.sample(frac=size_as_fraction)\n",
    "    return (sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Super Learner Class\n",
    "This class defined by the three functions: **initialises** class variables, by assigning their values by input commands or by default commands, and adding some self variables that are simply pawns in helping the whole funcitonality of the class; **fitting** model, by taking an X and Y dataset, combining, training base estimators on cross validated datasets of these and outputting predictions which are to be used in training the stack estimator on the resulting so-called Z dataset. The base estimators are then trained on bootstrapped samples of the whole training set. Finally **predicting** the response values of a test set by using predictions by base estimators and a stack model to predict the responses using these predictions. Note I constructed the predict function in similar form to the fit function, but frankly, there is absolutely no need for folds in the predict function, as there are no responses to fit to, so it would be more efficient to simply predict the entire test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SuperLearnerClassifier(BaseEstimator, ClassifierMixin):  \n",
    "    \"\"\"\n",
    "    SuperLearnerClassifier Class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model_list = ['dt','dt','dt'], \\\n",
    "                 stack_model = 'dt', type_output_labels = 'predictions', \\\n",
    "                 add_orgdata = 'yes', bootstrap_size = 0.8, rand_seed = 3):\n",
    "        self.base_model_list=base_model_list\n",
    "        self.stack_model=stack_model\n",
    "        self.type_output_labels=type_output_labels\n",
    "        self.add_orgdata=add_orgdata\n",
    "        self.bootstrap_size = bootstrap_size\n",
    "        self.rand_seed=rand_seed\n",
    "        self.num_base_estimators = len(self.base_model_list)\n",
    "        # Define stack model and base models (up to 10) to be used later\n",
    "        self.Zdata = None\n",
    "        self.ult_model = None\n",
    "        self.model_list = []\n",
    "        for i in range(1,11):\n",
    "            self.model_list.append('model'+str(i))\n",
    "        for model in self.model_list:\n",
    "            setattr(self, model, None)\n",
    "\n",
    "    def fit(self,X_train_data,Y_train_data):        \n",
    "        X_train_data = pd.DataFrame(data=X_train_data)\n",
    "        Y_train_data = pd.DataFrame(data=Y_train_data)\n",
    "        X_train_data = X_train_data.reset_index(drop=True)\n",
    "        Y_train_data = Y_train_data.reset_index(drop=True)\n",
    "        train_data = pd.merge(Y_train_data, X_train_data, left_index=True, right_index=True)\n",
    "        # so reset X and Y and merge to one so response values\n",
    "        # are in correct order so easy to train\n",
    "        folds_dict = Divide_Into_Folds(train_data,self.num_base_estimators,self.rand_seed)\n",
    "        # returns dict of folds, no of folds determined by no. of base estimators\n",
    "        for k in range(self.num_base_estimators):\n",
    "            other_keys = [m for m in folds_dict.keys() if m!=k+1]\n",
    "            appended_list = [folds_dict[m] for m in other_keys[1:]]\n",
    "            train_set = folds_dict[other_keys[0]].append(appended_list,ignore_index=True)\n",
    "            test_set = folds_dict[k+1]\n",
    "            # train set is amalgamated all folds other than k\n",
    "            # test set is kth fold\n",
    "            for n,each_model in enumerate(self.base_model_list):\n",
    "                model = BaseModelName(each_model)\n",
    "                # initialise new model according to string in list\n",
    "                model = model.fit(train_set[train_set.columns[1:]],train_set[train_set.columns[0]])\n",
    "                if self.type_output_labels == 'probabilities':\n",
    "                    y_pred = model.predict_proba(test_set[test_set.columns[1:]])\n",
    "                else:\n",
    "                    y_pred = model.predict(test_set[test_set.columns[1:]])\n",
    "                # if output labels are probabilities use predict_proba otherwise predict\n",
    "                y_pred = pd.DataFrame(data=y_pred)\n",
    "                if n+1 == 1:\n",
    "                    temp_pred = y_pred\n",
    "                else:\n",
    "                    temp_pred = pd.merge(temp_pred, y_pred, left_index=True, right_index=True)\n",
    "                # for each fold, merge predictions from each base estimator\n",
    "                # if 1st output labels then initialise, otherwise add to existing list\n",
    "            if k+1 == 1:\n",
    "                all_pred = temp_pred\n",
    "            else:\n",
    "                all_pred = all_pred.append(temp_pred,ignore_index=True)\n",
    "            # for each fold, merge predictions from last fold, to get set of predictions\n",
    "            # for all rows\n",
    "        full_data = folds_dict[1].append([folds_dict[m] for m in range(2,self.num_base_estimators+1)],ignore_index=False)\n",
    "        # combine folds to new dataset that is in order of predictions above\n",
    "        if self.add_orgdata == 'no':\n",
    "            responses = pd.DataFrame(full_data[full_data.columns[0]])\n",
    "            new_train = pd.merge(responses, all_pred, left_index=True, right_index=True)\n",
    "        else:\n",
    "            new_train = pd.merge(full_data, all_pred, left_index=True, right_index=True)\n",
    "        # if not using original data in dataset Z to accompany output labels of base estimators\n",
    "        # then merge with X data to get the right order then exclude those cols\n",
    "        # if use original data too then merge with X data\n",
    "        new_train.sort_index(inplace=True)\n",
    "        new_train.reset_index(drop=True)\n",
    "        # sort training set so outputted predictions are in same order as input data\n",
    "        self.ult_model = BaseModelName(self.stack_model)\n",
    "        self.ult_model = self.ult_model.fit(new_train[new_train.columns[1:]],new_train[new_train.columns[0]])\n",
    "        # initialise and set stack model\n",
    "        for m,each_model in enumerate(self.base_model_list):\n",
    "            setattr(self, 'model'+str(m+1), BaseModelName(each_model))\n",
    "            bootstrap_sample = Bootstrap(train_data,self.bootstrap_size)\n",
    "            setattr(self, 'model'+str(m+1), getattr(self, 'model'+str(m+1)).\\\n",
    "                        fit(bootstrap_sample[bootstrap_sample.columns[1:]],bootstrap_sample[bootstrap_sample.columns[0]]))\n",
    "        # train each of the base estimators on bootstrapped samples of original data\n",
    "        # to be used in predicting test set\n",
    "        return(self)\n",
    "\n",
    "    def predict(self,X_test_data):\n",
    "        # organised in very similar fashion to fit function so comments there\n",
    "        # hopefully satisfice to explain the thoughts behind the code\n",
    "        X_test_data = pd.DataFrame(data=X_test_data)\n",
    "        X_test_data = X_test_data.reset_index(drop=True)\n",
    "        folds_dict = Divide_Into_Folds(X_test_data,self.num_base_estimators,self.rand_seed)\n",
    "        for k in range(self.num_base_estimators):\n",
    "            test_set = folds_dict[k+1]\n",
    "            for n,model in enumerate(self.base_model_list):\n",
    "                if self.type_output_labels == 'probabilities':\n",
    "                    y_pred = getattr(self, 'model'+str(n+1)).predict_proba(test_set)\n",
    "                else:\n",
    "                    y_pred = getattr(self, 'model'+str(n+1)).predict(test_set)\n",
    "                y_pred = pd.DataFrame(data=y_pred)\n",
    "                if n+1 == 1:\n",
    "                    temp_pred = y_pred\n",
    "                else:\n",
    "                    temp_pred = pd.merge(temp_pred, y_pred, left_index=True, right_index=True)                \n",
    "            if k+1 == 1:\n",
    "                all_pred = temp_pred\n",
    "            else:\n",
    "                all_pred = all_pred.append(temp_pred,ignore_index=True)\n",
    "        full_data = folds_dict[1].append([folds_dict[m] for m in range(2,self.num_base_estimators+1)],ignore_index=False)\n",
    "        self.Zdata = pd.merge(full_data,all_pred, how='inner', left_index=True,right_index=True )\n",
    "        self.Zdata.sort_index(inplace=True)\n",
    "        self.Zdata.reset_index(drop=True)\n",
    "        if self.add_orgdata == 'no':\n",
    "            col = full_data.shape[1]\n",
    "            self.Zdata = self.Zdata[self.Zdata.columns[col:]]\n",
    "        predictions = self.ult_model.predict(self.Zdata)\n",
    "        return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = SuperLearnerClassifier(base_model_list = ['dt','lr','nb'], \\\n",
    "                            stack_model = 'dt',type_output_labels = 'predictions',\\\n",
    "                            add_orgdata = 'no', bootstrap_size = 0.8, rand_seed = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(sl, X_train, Y_train, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09702920038251743\n"
     ]
    }
   ],
   "source": [
    "print(sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code takes a long time to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare score to basic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tree = BaseModelName('dt')\n",
    "scores_dt = cross_val_score(dec_tree, X_train, Y_train, cv=3)\n",
    "print(sum(scores_dt)/len(scores_dt))  \n",
    "nav_bayes = BaseModelName('nb')\n",
    "scores_nb = cross_val_score(nav_bayes, X_train, Y_train, cv=3)\n",
    "print(sum(scores_nb)/len(scores_nb))  \n",
    "knn = BaseModelName('knn')\n",
    "scores_knn = cross_val_score(knn, X_train, Y_train, cv=3)\n",
    "print(sum(scores_knn)/len(scores_knn)) \n",
    "mlp = BaseModelName('mlp')\n",
    "scores_mlp = cross_val_score(mlp, X_train, Y_train, cv=3)\n",
    "print(sum(scores_mlp)/len(scores_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find best combination of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Powerset(iterable):\n",
    "    \"\"\"\n",
    "    Returns powerset by with restraint that no. of elements >= 2\n",
    "    so Powerset([1,2,3]) --> (1,2) (1,3) (2,3) (1,2,3)\n",
    "    \"\"\"\n",
    "    constituents = list(iterable)\n",
    "    return chain.from_iterable(combinations(constituents,n) \\\n",
    "                               for n in range(2,len(constituents)+1))\n",
    "\n",
    "all_possible_base_model_combos = list(Powerset(['dt','lr','knn','mlp','nb'])) # svm omitted due to no predict_proba() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note I omitted svm as it did not have the predict_proba() function for some reason.I did not run the grid search through all possible combinations of base estimator alliances as that would take an inordinate length of time to complete (342 combinations of >2 different base estimators when combined with iterating over all the values of stack estimators and output labels). So I randomly sampled from each number of base estimators (so for example 2 sampled combinations of base estimators could be ['dt','knn'] and ['mlp',nb']); 2 combinations of 2 base estimators, 2 of 3, 2 of 4, and 1 of 5. The higher combinations having less samples as there are less of them. The grid search also iterated over stack estimators (7), and outputs to train the stack model on (2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combinations = [list(combo) for combo in all_possible_base_model_combos]\n",
    "models2, models3, models4, models5 = [], [], [], []\n",
    "for i in range(2,6):\n",
    "    for combo in all_combinations:\n",
    "        if len(combo)==i:\n",
    "            locals()[\"models\"+str(i)] += [combo]\n",
    "\n",
    "models2 = [models2[j] for j in sorted(random.sample(range(len(models2)), 2))]\n",
    "models3 = [models3[j] for j in sorted(random.sample(range(len(models3)), 2))]\n",
    "models4 = [models4[j] for j in sorted(random.sample(range(len(models4)), 2))]\n",
    "# models5 only contains one element so no need to sample from\n",
    "        \n",
    "some_combinations = models2+models3+models4+models5\n",
    "param_grid ={'base_model_list': [model_list for model_list in some_combinations], \\\n",
    "             'stack_model': ['dt','knn','nb','lr','svm','mlp'], \\\n",
    "             'type_output_labels': ['predictions','probabilities'],\\\n",
    "             'bootstrap_size' : [0.65],\n",
    "             'add_orgdata': ['no'], 'rand_seed': [3]}\n",
    "grid_search = GridSearchCV(SuperLearnerClassifier(), param_grid, cv=2, \\\n",
    "                             verbose = 1, return_train_score=True)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "print(\"Best parameters set found on development set:\")\n",
    "display(grid_search.best_params_)\n",
    "display(grid_search.best_score_)\n",
    "display(grid_search.cv_results_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare performance when training on the original data values or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sl_withorgdata = SuperLearnerClassifier(base_model_list = ['dt','lr','nb'], \\\n",
    "                            stack_model = 'nb',type_output_labels = 'predictions',\\\n",
    "                            add_orgdata = 'yes', bootstrap_size = 0.65, rand_seed = 30)\n",
    "sl_withorgdata = sl_withorgdata.fit(X_train, Y_train)\n",
    "y_sl_withorgdata = sl_withorgdata.predict(X_test)\n",
    "print(metrics.accuracy_score(Y_test, y_sl_withorgdata))\n",
    "sl_withoutorgdata = SuperLearnerClassifier(base_model_list = ['dt','lr','nb'], \\\n",
    "                            stack_model = 'nb',type_output_labels = 'predictions',\\\n",
    "                            add_orgdata = 'no', bootstrap_size = 0.65, rand_seed = 30)\n",
    "sl_withoutorgdata = sl_withoutorgdata.fit(X_train, Y_train)\n",
    "y_sl_withoutorgdata = sl_withoutorgdata.predict(X_test)\n",
    "print(metrics.accuracy_score(Y_test, y_sl_withoutorgdata))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
